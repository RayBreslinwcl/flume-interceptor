1.pom文件
2.定义flume拦截器
本项目中自定义了两个拦截器，分别是：ETL拦截器、日志类型区分拦截器。
（1）ETL拦截器LogETLInterceptor：主要用于，过滤时间戳不合法和Json数据不完整的日志
（2）日志类型区分拦截器LogTypeInterceptor：主要用于，将启动日志和事件日志区分开来，方便发往Kafka的不同Topic。

3.打包
maven->package
获得瘦包就可以，因为相关依赖在服务器flume的lib下都存在
"..\bdutil\flume-interceptor\target\interceptor-1.0-SNAPSHOT.jar"

4.部署
（1）因为cdh集群，所以上传到即可
/opt/cloudera/parcels/CDH-5.15.2-1.cdh5.15.2.p0.3/lib/flume-ng/lib
（2）分配到cdh2和cdh3各有一个

5.cdh配置flume-kafka
flume收集数据传输至kafka
（1）第一步：修改代理Agent Name为a1

（2）第二步：配置写入如下
a1.sources=r1
a1.channels=c1 c2
a1.sinks=k1 k2

# configure source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/module/flume/log_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /tmp/logs/*.log
a1.sources.r1.fileHeader = true
a1.sources.r1.channels = c1 c2

#interceptor
a1.sources.r1.interceptors = i1 i2
a1.sources.r1.interceptors.i1.type = com.bd.flume.interceptor.LogETLInterceptor
a1.sources.r1.interceptors.i2.type = com.bd.flume.interceptor.LogTypeInterceptor

# selector
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_start = c1
a1.sources.r1.selector.mapping.topic_event = c2

# configure channel
a1.channels.c1.type = memory
a1.channels.c1.capacity=10000
a1.channels.c1.byteCapacityBufferPercentage=20

a1.channels.c2.type = memory
a1.channels.c2.capacity=10000
a1.channels.c2.byteCapacityBufferPercentage=20

# configure sink
# start-sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.topic = topic_start
a1.sinks.k1.kafka.bootstrap.servers = cdh1:9092,cdh2:9092,cdh3:9092
a1.sinks.k1.kafka.flumeBatchSize = 2000
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.channel = c1

# event-sink
a1.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k2.kafka.topic = topic_event
a1.sinks.k2.kafka.bootstrap.servers = cdh1:9092,cdh2:9092,cdh3:9092
a1.sinks.k2.kafka.flumeBatchSize = 2000
a1.sinks.k2.kafka.producer.acks = 1
a1.sinks.k2.channel = c2

注意：com.atguigu.flume.interceptor.LogETLInterceptor和com.atguigu.flume.interceptor.LogTypeInterceptor是自定义的拦截器的全类名。需要根据用户自定义的拦截器做相应修改。

（3）修改/opt/module/flume/log_position.json的读写权限
[root@hadoop102 module]# mkdir -p /opt/module/flume
[root@hadoop102 flume]# touch log_position.json
[root@hadoop102 flume]# chmod 777 log_position.json
[root@hadoop102 module]# xsync flume/

【测试：】
[root@cdh2 logs]# mkdir -p /opt/module/flume
[root@cdh2 logs]# cd /opt/module/flume/
[root@cdh2 flume]# touch log_position.json
[root@cdh2 flume]# chmod 777 log_position.json
[root@cdh2 flume]# cd ..
[root@cdh2 module]# scp -r flume/ root@cdh3:/opt/module/
log_position.json

注意：Json文件的父目录一定要创建好，并改好权限


6.Flume将kafka数据放入hdsf
（1）在CM管理页面cdh3上Flume的配置中找到代理名称
a1


（2）在配置文件如下内容(kafka-hdfs)
## 组件
a1.sources=r1 r2
a1.channels=c1 c2
a1.sinks=k1 k2

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = cdh1:9092,cdh2:9092,cdh3:9092
a1.sources.r1.kafka.topics=topic_start

## source2
a1.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.batchSize = 5000
a1.sources.r2.batchDurationMillis = 2000
a1.sources.r2.kafka.bootstrap.servers = cdh1:9092,cdh2:9092,cdh3:9092
a1.sources.r2.kafka.topics=topic_event

## channel1
a1.channels.c1.type=memory
a1.channels.c1.capacity=100000
a1.channels.c1.transactionCapacity=10000

## channel2
a1.channels.c2.type=memory
a1.channels.c2.capacity=100000
a1.channels.c2.transactionCapacity=10000

## sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_start/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = logstart-
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = second

##sink2
a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = /origin_data/gmall/log/topic_event/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = logevent-
a1.sinks.k2.hdfs.round = true
a1.sinks.k2.hdfs.roundValue = 10
a1.sinks.k2.hdfs.roundUnit = second

## 不要产生大量小文件
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

a1.sinks.k2.hdfs.rollInterval = 10
a1.sinks.k2.hdfs.rollSize = 134217728
a1.sinks.k2.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k2.hdfs.fileType = CompressedStream

a1.sinks.k1.hdfs.codeC = snappy
a1.sinks.k2.hdfs.codeC = snappy

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1

a1.sources.r2.channels = c2
a1.sinks.k2.channel= c2

因为本地cdh环境，暂时不支持lzo
[root@cdh3 softwares]# hadoop checknative
20/03/15 16:45:09 INFO bzip2.Bzip2Factory: Successfully loaded & initialized native-bzip2 library system-native
20/03/15 16:45:09 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
Native library checking:
hadoop:  true /opt/cloudera/parcels/CDH-5.15.2-1.cdh5.15.2.p0.3/lib/hadoop/lib/native/libhadoop.so.1.0.0
zlib:    true /lib64/libz.so.1
snappy:  true /opt/cloudera/parcels/CDH-5.15.2-1.cdh5.15.2.p0.3/lib/hadoop/lib/native/libsnappy.so.1
lz4:     true revision:10301
bzip2:   true /lib64/libbz2.so.1
openssl: true /lib64/libcrypto.so
